{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"09-20-24\"\n",
    "data_location = rf\"E:\\Project 6 - Temperature\\Experiments\\data_analysis\\{date}\"\n",
    "data_is_cleaned = [\"Yes\", \"No\", \"Partially\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all peaks/troughs into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cleaned data for position 0\n",
      "Using cleaned data for position 1\n",
      "Using raw data for position 2\n",
      "No flagged data for position 2\n",
      "Using raw data for position 3\n",
      "No flagged data for position 3\n",
      "Using raw data for position 4\n",
      "No flagged data for position 4\n",
      "Using raw data for position 5\n",
      "No flagged data for position 5\n",
      "Using raw data for position 6\n",
      "No flagged data for position 6\n",
      "Using cleaned data for position 7\n",
      "Using cleaned data for position 8\n",
      "Using cleaned data for position 9\n",
      "Using cleaned data for position 10\n",
      "Using cleaned data for position 11\n",
      "Using cleaned data for position 12\n",
      "Using cleaned data for position 13\n",
      "Using cleaned data for position 14\n",
      "Using cleaned data for position 15\n",
      "Using cleaned data for position 16\n",
      "Using cleaned data for position 17\n",
      "Using cleaned data for position 18\n",
      "Using cleaned data for position 19\n",
      "Using raw data for position 20\n",
      "No flagged data for position 20\n",
      "Using cleaned data for position 21\n",
      "Using cleaned data for position 22\n",
      "Using cleaned data for position 23\n",
      "Using cleaned data for position 24\n",
      "Using cleaned data for position 25\n",
      "Using cleaned data for position 26\n",
      "Using cleaned data for position 27\n",
      "Using raw data for position 28\n",
      "No flagged data for position 28\n",
      "Using cleaned data for position 29\n",
      "Using cleaned data for position 30\n",
      "Using cleaned data for position 31\n",
      "Using cleaned data for position 32\n",
      "Using cleaned data for position 33\n",
      "Using cleaned data for position 34\n",
      "Using cleaned data for position 35\n",
      "Using cleaned data for position 36\n",
      "Using cleaned data for position 37\n",
      "Using cleaned data for position 38\n",
      "Using cleaned data for position 39\n",
      "Using cleaned data for position 40\n",
      "Using raw data for position 41\n",
      "No flagged data for position 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tavel\\AppData\\Local\\Temp\\ipykernel_16996\\808174308.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_peaks_and_troughs_df = pd.concat([all_peaks_and_troughs_df, peaks_and_troughs_df])\n"
     ]
    }
   ],
   "source": [
    "all_peaks_and_troughs_df = pd.DataFrame()\n",
    "total_positions = 42\n",
    "positions = range(total_positions)\n",
    "# positions = [i for i in range(13)] + [i + 26 for i in range(13)]\n",
    "for pos in positions:\n",
    "    # Load peaks and troughs\n",
    "    if data_is_cleaned == \"Yes\":\n",
    "        peaks_and_troughs_df = pd.read_csv(data_location + f\"\\Pos{pos}_curated_peaks_and_troughs.csv\")\n",
    "    elif data_is_cleaned == \"No\":\n",
    "        peaks_and_troughs_df = pd.read_csv(data_location + f\"\\Pos{pos}_peaks_and_troughs.csv\")\n",
    "    elif data_is_cleaned == \"Partially\":\n",
    "        try:\n",
    "            peaks_and_troughs_df = pd.read_csv(data_location + f\"\\Pos{pos}_curated_peaks_and_troughs.csv\")\n",
    "            print(f\"Using cleaned data for position {pos}\")\n",
    "        except:\n",
    "            peaks_and_troughs_df = pd.read_csv(data_location + f\"\\Pos{pos}_peaks_and_troughs.csv\")\n",
    "            print(f\"Using raw data for position {pos}\")\n",
    "    try:\n",
    "        # Load flagged data\n",
    "        flagged_df = pd.read_csv(data_location + f\"\\Pos{pos}_flagged_data.csv\")\n",
    "        # Remove TRACK_IDs on flagged data from peaks and troughs\n",
    "        peaks_and_troughs_df = peaks_and_troughs_df[~peaks_and_troughs_df[\"TRACK_ID\"].isin(flagged_df[\"TRACK_ID\"])]\n",
    "    except:\n",
    "        print(f\"No flagged data for position {pos}\")\n",
    "    # Add position column\n",
    "    peaks_and_troughs_df[\"POSITION\"] = pos\n",
    "    # Add to all_peaks_and_troughs_df\n",
    "    all_peaks_and_troughs_df = pd.concat([all_peaks_and_troughs_df, peaks_and_troughs_df])\n",
    "# Reset index\n",
    "all_peaks_and_troughs_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix any NaN values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is a track that contains NaNs in the peaks and troughs, flag it\n",
    "# If it contains NaNs in the cycle numbers, re-number that track\n",
    "flagged_tracks = {\n",
    "    'type_nan': [],\n",
    "    'time_nan': [],\n",
    "    'peak_cycle_renumbering': [],\n",
    "    'trough_cycle_renumbering': [],\n",
    "    'indexes_to_remove': [],\n",
    "}\n",
    "# Do it for each position\n",
    "for position in all_peaks_and_troughs_df[\"POSITION\"].unique():\n",
    "    position_tracks = all_peaks_and_troughs_df[all_peaks_and_troughs_df[\"POSITION\"] == position]\n",
    "    # Check if there are NaNs on the track id\n",
    "    if position_tracks[\"TRACK_ID\"].isnull().values.any():\n",
    "        # Get specific index where this happens\n",
    "        indexes_to_remove = position_tracks[position_tracks[\"TRACK_ID\"].isnull()].index.tolist()\n",
    "        flagged_tracks[\"indexes_to_remove\"].extend(indexes_to_remove)\n",
    "    for track_id in position_tracks[\"TRACK_ID\"].unique():\n",
    "        track_df = position_tracks[position_tracks[\"TRACK_ID\"] == track_id]\n",
    "        # Check if there are NaNs\n",
    "        if track_df.isnull().values.any():\n",
    "            if track_df[\"TIME\"].isnull().values.any():\n",
    "                # NaN is in time or track id, remove\n",
    "                flagged_tracks[\"time_nan\"].append((position, track_id))\n",
    "            elif track_df[\"TYPE\"].isnull().values.any():\n",
    "                # NaN is in the type column\n",
    "                flagged_tracks[\"type_nan\"].append((position, track_id))\n",
    "            elif track_df[\"CYCLE\"].isnull().values.any():\n",
    "                # Check if nan is on a peak or trough\n",
    "                peak_cycle_numbers = track_df[track_df[\"TYPE\"] == \"PEAK\"][\"CYCLE\"].values\n",
    "                trough_cycle_numbers = track_df[track_df[\"TYPE\"] == \"TROUGH\"][\"CYCLE\"].values\n",
    "                if np.isnan(peak_cycle_numbers).any():\n",
    "                    # NaN is in the peak cycle numbers\n",
    "                    flagged_tracks[\"peak_cycle_renumbering\"].append((position, track_id))\n",
    "                if np.isnan(trough_cycle_numbers).any():\n",
    "                    # NaN is in the trough cycle numbers\n",
    "                    flagged_tracks[\"trough_cycle_renumbering\"].append((position, track_id))\n",
    "        else:\n",
    "            # Check that peak and troughs cycle numbering is continous\n",
    "            peak_cycle_numbers = track_df[track_df[\"TYPE\"] == \"PEAK\"][\"CYCLE\"].values\n",
    "            trough_cycle_numbers = track_df[track_df[\"TYPE\"] == \"TROUGH\"][\"CYCLE\"].values\n",
    "            if np.any(np.diff(peak_cycle_numbers) != 1):\n",
    "                # Peak cycle numbers are not continous\n",
    "                flagged_tracks['peak_cycle_renumbering'].append((position, track_id))\n",
    "            if np.any(np.diff(trough_cycle_numbers) != 1):\n",
    "                # Trough cycle numbers are not continous\n",
    "                flagged_tracks['trough_cycle_renumbering'].append((position, track_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type_nan: 201\n",
      "time_nan: 3\n",
      "peak_cycle_renumbering: 0\n",
      "trough_cycle_renumbering: 0\n",
      "indexes_to_remove: 2\n"
     ]
    }
   ],
   "source": [
    "# Print how many flagged types per category there are\n",
    "for key, value in flagged_tracks.items():\n",
    "    print(f\"{key}: {len(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_peaks_and_troughs_df = all_peaks_and_troughs_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix flagged tracks: peak_cycle_renumbering\n",
    "for i in range(len(flagged_tracks[\"peak_cycle_renumbering\"])):\n",
    "    position, track_id = flagged_tracks[\"peak_cycle_renumbering\"][i]\n",
    "    track_df = all_peaks_and_troughs_df[(all_peaks_and_troughs_df[\"POSITION\"] == position) & (all_peaks_and_troughs_df[\"TRACK_ID\"] == track_id)]\n",
    "    # Renumber peak cycle numbers\n",
    "    # get current peak order based on time\n",
    "    # print(track_df[track_df[\"TYPE\"] == \"PEAK\"])\n",
    "    peak_cycle_numbers = track_df[track_df[\"TYPE\"] == \"PEAK\"][\"CYCLE\"].values\n",
    "    peak_cycle_numbers = np.arange(1, len(peak_cycle_numbers) + 1)\n",
    "    # Update the processed_peaks_and_troughs_df\n",
    "    indexes_to_renumber = track_df[track_df[\"TYPE\"] == \"PEAK\"].sort_values(\"TIME\").index\n",
    "    processed_peaks_and_troughs_df.loc[indexes_to_renumber, \"CYCLE\"] = peak_cycle_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix flagged tracks: trough_cycle_renumbering\n",
    "for i in range(len(flagged_tracks[\"trough_cycle_renumbering\"])):\n",
    "    position, track_id = flagged_tracks[\"trough_cycle_renumbering\"][i]\n",
    "    track_df = all_peaks_and_troughs_df[(all_peaks_and_troughs_df[\"POSITION\"] == position) & (all_peaks_and_troughs_df[\"TRACK_ID\"] == track_id)]\n",
    "    # Renumber trough cycle numbers\n",
    "    trough_cycle_numbers = track_df[track_df[\"TYPE\"] == \"TROUGH\"][\"CYCLE\"].values\n",
    "    trough_cycle_numbers = np.arange(1, len(trough_cycle_numbers) + 1)\n",
    "    # Update the processed_peaks_and_troughs_df\n",
    "    indexes_to_renumber = track_df[track_df[\"TYPE\"] == \"TROUGH\"].sort_values(\"TIME\").index\n",
    "    processed_peaks_and_troughs_df.loc[indexes_to_renumber, \"CYCLE\"] = trough_cycle_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix flagged tracks: type_nan\n",
    "ids_without_comparison = []\n",
    "positions_without_comparison = []\n",
    "\n",
    "# Replace nan with the type of the closest peak or trough in index\n",
    "for i in range(len(flagged_tracks[\"type_nan\"])):\n",
    "    position, track_id = flagged_tracks[\"type_nan\"][i]\n",
    "    track_df = all_peaks_and_troughs_df[(all_peaks_and_troughs_df[\"POSITION\"] == position) & (all_peaks_and_troughs_df[\"TRACK_ID\"] == track_id)]\n",
    "    # Get the indexes of the nans\n",
    "    indexes_to_replace = track_df[track_df[\"TYPE\"].isnull()].index\n",
    "    # Get the indexes of the peaks and troughs\n",
    "    peaks_indexes = track_df[track_df[\"TYPE\"] == \"PEAK\"].index\n",
    "    troughs_indexes = track_df[track_df[\"TYPE\"] == \"TROUGH\"].index\n",
    "    # Replace the nans with the type of the closest peak or trough\n",
    "    for index in indexes_to_replace:\n",
    "        try:\n",
    "            # Get the closest peak and trough\n",
    "            closest_peak_index = peaks_indexes[np.abs(peaks_indexes - index).argmin()]\n",
    "            closest_trough_index = troughs_indexes[np.abs(troughs_indexes - index).argmin()]\n",
    "        except:\n",
    "            ids_without_comparison.append(track_id)\n",
    "            positions_without_comparison.append(position)\n",
    "            continue\n",
    "        # Get the type of the closest peak or trough\n",
    "        if abs(closest_peak_index - index) < abs(closest_trough_index - index):\n",
    "            type_of_closest = \"PEAK\"\n",
    "        else:\n",
    "            type_of_closest = \"TROUGH\"\n",
    "        # Replace the nan with the type of the closest peak or trough\n",
    "        processed_peaks_and_troughs_df.loc[index, \"TYPE\"] = type_of_closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 48 tracks without comparison in positions [37 38 39]\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(ids_without_comparison)} tracks without comparison in positions {np.unique(positions_without_comparison)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_cycle_numbers_to_peak_types(cycle_numbers):\n",
    "    peak_types = np.array([\"\"] * len(cycle_numbers), dtype=object)\n",
    "    diff = np.diff(cycle_numbers, prepend=0)\n",
    "    switch_place = np.where(diff <= 0)[0]\n",
    "    peak_range = range(switch_place[0]) if len(switch_place) > 0 else 0\n",
    "    peak_types[peak_range] = \"PEAK\"\n",
    "    trough_range = range(switch_place[0], len(cycle_numbers)) if len(switch_place) > 0 else []\n",
    "    peak_types[trough_range] = \"TROUGH\"\n",
    "    return peak_types\n",
    "\n",
    "cycle_numbers = [1, 1]\n",
    "peak_types = map_cycle_numbers_to_peak_types(cycle_numbers)\n",
    "assert np.all(peak_types == [\"PEAK\", \"TROUGH\"])\n",
    "\n",
    "cycle_numbers = [1]\n",
    "peak_types = map_cycle_numbers_to_peak_types(cycle_numbers)\n",
    "assert np.all(peak_types == [\"PEAK\"])\n",
    "\n",
    "cycle_numbers = [1, 2, 3, 1, 2, 3]\n",
    "peak_types = map_cycle_numbers_to_peak_types(cycle_numbers)\n",
    "assert np.all(peak_types == [\"PEAK\", \"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\", \"TROUGH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [TRACK_ID, TIME, RATIO, TYPE, CYCLE, POSITION]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "pos = 11\n",
    "track_id = 0\n",
    "track_df = all_peaks_and_troughs_df[(all_peaks_and_troughs_df[\"POSITION\"] == pos) & (all_peaks_and_troughs_df[\"TRACK_ID\"] == track_id)]\n",
    "print(track_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "values_to_set = []\n",
    "\n",
    "# Manually fix some NaNs if necessary\n",
    "if date == \"03-29-24\":\n",
    "    indexes = [\n",
    "        (6, 156), # position, track\n",
    "        (7, 6), (7, 51), (7, 60), (7, 64),\n",
    "        (20, 2), (20, 3), (20, 10), (20, 11), (20, 12), (20, 15),\n",
    "        (20, 31), (20, 41), (20, 43), (20, 47), (20, 49), (20, 52), (20, 58), (20, 61),\n",
    "        (20, 62), (20, 63), (20, 76), (21, 35), (21, 36), (21, 44),\n",
    "        (21, 47), (21, 49), (21, 59), (21, 74), (21, 93), (21, 97),\n",
    "        (21, 118), (21, 120), (21, 136), (21, 143), (21, 149), (21, 158), (21, 159), \n",
    "        (21, 163), (21, 166), (21, 181),\n",
    "        (22, 0), (23, 40), (23, 43), (23, 47), (23, 77), (23, 200), (23, 203), (23, 227),\n",
    "        (23, 229), (24, 123), (24, 129), (24, 132), (24, 154), (24, 160),\n",
    "        (26, 94),\n",
    "\n",
    "    ]\n",
    "    values_to_set = [\n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\"], [\"PEAK\", \"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\"], [\"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\"], [\"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\"], [\"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\"], [\"PEAK\", \"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"PEAK\", \"TROUGH\"], [\"PEAK\", \"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\"] * 8 + [\"TROUGH\"] * 7, [\"PEAK\"] * 13 + [\"TROUGH\"] * 12,\n",
    "        [\"PEAK\"] * 14 + [\"TROUGH\"] * 13, [\"PEAK\"] * 10 + [\"TROUGH\"] * 10,\n",
    "        [\"PEAK\"] * 12 + [\"TROUGH\"] * 12, [\"PEAK\"] * 11 + [\"TROUGH\"] * 11, [\"PEAK\"] * 13 + [\"TROUGH\"] * 13,\n",
    "        [\"PEAK\"] * 10 + [\"TROUGH\"] * 9,\n",
    "        [\"PEAK\"] * 9 + [\"TROUGH\"] * 8,\n",
    "        [\"PEAK\"] * 3 + [\"TROUGH\"] * 2, [\"PEAK\", \"PEAK\", \"TROUGH\"], [\"PEAK\", \"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\"] * 3 + [\"TROUGH\"] * 3,\n",
    "\n",
    "    ]\n",
    "\n",
    "if date == \"08-09-24\":\n",
    "    indexes = [\n",
    "        (0, 15), # position, track\n",
    "        (0, 31), (0, 39), (0, 47), (0, 49), (0, 237), (0, 363),\n",
    "\n",
    "        (1, 8), (1, 10), (1, 38), (1, 116), (1, 170),\n",
    "\n",
    "        (5, 12), (6, 53), \n",
    "\n",
    "        # here\n",
    "        (11, 11), (11, 23), (11, 30), (11, 33),\n",
    "        (11, 36), (11, 42), (11, 76), (11, 181),\n",
    "        (11, 224), (11, 227),\n",
    "\n",
    "        (12, 6), (12, 8), (12, 9), (12, 14), (12, 16), (12, 17),\n",
    "        (12, 26), (12, 27), (12, 32), (12, 33), (12, 34), (12, 35),\n",
    "        (12, 37), (12, 40), (12, 41), (12, 44), (12, 46), (12, 47),\n",
    "        (12, 52), (12, 53), (12, 55), (12, 60), (12, 63), (12, 67),\n",
    "        (12, 69), (12, 70), (12, 72), (12, 74), (12, 76), (12, 79),\n",
    "        (12, 86), (12, 87), (12, 88), (12, 89), (12, 90), (12, 92),\n",
    "        (12, 94), (12, 96), (12, 98), (12, 99), (12, 100), (12, 101),\n",
    "        (12, 103), (12, 105), (12, 107), (12, 109),\n",
    "\n",
    "        (13, 4), \n",
    "\n",
    "        (15, 34), (15, 258), (15, 278), (15, 310), (15, 458), (15, 540),\n",
    "\n",
    "        \n",
    "        \n",
    "        (17, 61), (17, 215), (17, 231), (18, 96),\n",
    "        (20, 4), (20, 19), (23, 81), (25, 135), (26, 9), (26, 91),\n",
    "        (26, 153), (26, 293), (26, 300), (26, 303),\n",
    "    ]\n",
    "    values_to_set = [\n",
    "        [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"],\n",
    "\n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\",  \"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\"],\n",
    "\n",
    "        [\"PEAK\", \"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "\n",
    "        # here\n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "        # 12\n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], \n",
    "\n",
    "        # 13\n",
    "        [\"PEAK\", \"TROUGH\"],\n",
    "        # 15\n",
    "        [\"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "\n",
    "\n",
    "        \n",
    "        [\"PEAK\", \"PEAK\", \"PEAK\", \"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"PEAK\", \"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"PEAK\", \"PEAK\", \"PEAK\", \"TROUGH\", \"TROUGH\", \"TROUGH\", \"TROUGH\"],\n",
    "        [\"PEAK\", \"TROUGH\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\",  \"PEAK\", \"TROUGH\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\",  \"PEAK\", \"TROUGH\", \"TROUGH\"], [\"PEAK\", \"TROUGH\"], \n",
    "        [\"PEAK\",  \"PEAK\", \"TROUGH\", \"TROUGH\"],\n",
    "    ]\n",
    "\n",
    "if date == \"08-23-24\":\n",
    "    indexes = []\n",
    "    values_to_set = []\n",
    "\n",
    "    for i in range(len(ids_without_comparison)):\n",
    "        position = positions_without_comparison[i]\n",
    "        track_id = ids_without_comparison[i]\n",
    "        track_df = all_peaks_and_troughs_df[(all_peaks_and_troughs_df[\"POSITION\"] == position) &\n",
    "                                            (all_peaks_and_troughs_df[\"TRACK_ID\"] == track_id)] \n",
    "        indexes.append((position, track_id))\n",
    "        peak_types = map_cycle_numbers_to_peak_types(track_df[\"CYCLE\"].values)\n",
    "        values_to_set.append(peak_types)\n",
    "\n",
    "if date == \"08-28-24\":\n",
    "    indexes = []\n",
    "    values_to_set = []\n",
    "\n",
    "    for i in range(len(ids_without_comparison)):\n",
    "        position = positions_without_comparison[i]\n",
    "        track_id = ids_without_comparison[i]\n",
    "        track_df = all_peaks_and_troughs_df[(all_peaks_and_troughs_df[\"POSITION\"] == position) &\n",
    "                                            (all_peaks_and_troughs_df[\"TRACK_ID\"] == track_id)] \n",
    "        indexes.append((position, track_id))\n",
    "        peak_types = map_cycle_numbers_to_peak_types(track_df[\"CYCLE\"].values)\n",
    "        values_to_set.append(peak_types)\n",
    "\n",
    "if date == \"08-16-24\":\n",
    "    indexes = []\n",
    "    values_to_set = []\n",
    "\n",
    "    for i in range(len(ids_without_comparison)):\n",
    "        position = positions_without_comparison[i]\n",
    "        track_id = ids_without_comparison[i]\n",
    "        track_df = all_peaks_and_troughs_df[(all_peaks_and_troughs_df[\"POSITION\"] == position) &\n",
    "                                            (all_peaks_and_troughs_df[\"TRACK_ID\"] == track_id)] \n",
    "        indexes.append((position, track_id))\n",
    "        peak_types = map_cycle_numbers_to_peak_types(track_df[\"CYCLE\"].values)\n",
    "        values_to_set.append(peak_types)\n",
    "\n",
    "if date == \"09-20-24\":\n",
    "    indexes = []\n",
    "    values_to_set = []\n",
    "\n",
    "    for i in range(len(ids_without_comparison)):\n",
    "        position = positions_without_comparison[i]\n",
    "        track_id = ids_without_comparison[i]\n",
    "        track_df = all_peaks_and_troughs_df[(all_peaks_and_troughs_df[\"POSITION\"] == position) &\n",
    "                                            (all_peaks_and_troughs_df[\"TRACK_ID\"] == track_id)] \n",
    "        indexes.append((position, track_id))\n",
    "        peak_types = map_cycle_numbers_to_peak_types(track_df[\"CYCLE\"].values)\n",
    "        values_to_set.append(peak_types)\n",
    "    \n",
    "for position, track in indexes:\n",
    "    index_location = processed_peaks_and_troughs_df[(processed_peaks_and_troughs_df[\"POSITION\"] == position) & (processed_peaks_and_troughs_df[\"TRACK_ID\"] == track)].index\n",
    "    processed_peaks_and_troughs_df.loc[index_location, \"TYPE\"] = values_to_set.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix flagged tracks: time_nan\n",
    "indexes_to_remove = []\n",
    "# For each one, check which index has NaN on time and remove those indexes\n",
    "for i in range(len(flagged_tracks[\"time_nan\"])):\n",
    "    position, track_id = flagged_tracks[\"time_nan\"][i]\n",
    "    track_df = all_peaks_and_troughs_df[(all_peaks_and_troughs_df[\"POSITION\"] == position) & (all_peaks_and_troughs_df[\"TRACK_ID\"] == track_id)]\n",
    "    # Check which indexes have NaNs on time\n",
    "    if track_df[\"TIME\"].isnull().values.any():\n",
    "        indexes_to_remove += track_df[track_df[\"TIME\"].isnull()].index.to_list()\n",
    "# Add indexes flagged for removal\n",
    "indexes_to_remove += flagged_tracks[\"indexes_to_remove\"]\n",
    "# Remove those indexes\n",
    "processed_peaks_and_troughs_df = processed_peaks_and_troughs_df.drop(index=indexes_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no more NaNs\n"
     ]
    }
   ],
   "source": [
    "# Check that there are no more nans\n",
    "if processed_peaks_and_troughs_df.isnull().values.any():\n",
    "    print(\"There are still NaNs\")\n",
    "    # print the indexes of the NaNs\n",
    "    print(processed_peaks_and_troughs_df[processed_peaks_and_troughs_df.isnull().any(axis=1)])\n",
    "else:\n",
    "    print(\"There are no more NaNs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually fix remaining NaNs if necessary\n",
    "if date == \"11-04-22\":\n",
    "    feature = \"CYCLE\"\n",
    "    indexes = [1310, 1683, 6921]\n",
    "    values_to_set = [5.0, 3.0, 1.0]\n",
    "\n",
    "    for i in range(len(indexes)):\n",
    "        processed_peaks_and_troughs_df.loc[indexes[i], feature] = values_to_set[i]\n",
    "\n",
    "if date == \"03-29-24\":\n",
    "    feature = \"TYPE\"\n",
    "    indexes = processed_peaks_and_troughs_df[processed_peaks_and_troughs_df.isnull().any(axis=1)].index\n",
    "    processed_peaks_and_troughs_df.loc[indexes, feature] = \"PEAK\"\n",
    "\n",
    "if date == \"08-09-24\":\n",
    "    feature = \"CYCLE\"\n",
    "    indexes = [1248]\n",
    "    for index in indexes:\n",
    "        processed_peaks_and_troughs_df.loc[index, feature] = 1.0\n",
    "\n",
    "    feature = \"TYPE\"\n",
    "    indexes = [4647, 4969]\n",
    "    values_to_set = [\"TROUGH\", \"PEAK\"]\n",
    "    for i in range(len(indexes)):\n",
    "        processed_peaks_and_troughs_df.loc[indexes[i], feature] = values_to_set[i]\n",
    "\n",
    "if date == \"08-23-24\":\n",
    "    feature = \"CYCLE\"\n",
    "    indexes = [5321]\n",
    "    values_to_set = [15.0]\n",
    "    for i in range(len(indexes)):\n",
    "        processed_peaks_and_troughs_df.loc[indexes[i], feature] = values_to_set[i]\n",
    "\n",
    "if date == \"08-28-24\":\n",
    "    feature = \"TYPE\"\n",
    "    indexes = processed_peaks_and_troughs_df[processed_peaks_and_troughs_df.isnull().any(axis=1)].index\n",
    "    values_to_set = [\"PEAK\"] * len(indexes)\n",
    "    for i in range(len(indexes)):\n",
    "        processed_peaks_and_troughs_df.loc[indexes[i], feature] = values_to_set[i]\n",
    "\n",
    "if date == \"08-16-24\":\n",
    "    feature = \"TYPE\"\n",
    "    indexes = processed_peaks_and_troughs_df[processed_peaks_and_troughs_df.isnull().any(axis=1)].index\n",
    "    values_to_set = [\"TROUGH\"] * len(indexes)\n",
    "    for i in range(len(indexes)):\n",
    "        processed_peaks_and_troughs_df.loc[indexes[i], feature] = values_to_set[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRACK_ID    0\n",
      "TIME        0\n",
      "RATIO       0\n",
      "TYPE        0\n",
      "CYCLE       0\n",
      "POSITION    0\n",
      "dtype: int64\n",
      "There are no more NaNs\n"
     ]
    }
   ],
   "source": [
    "print(processed_peaks_and_troughs_df.isnull().sum())\n",
    "# Check that there are no more nans\n",
    "if processed_peaks_and_troughs_df.isnull().values.any():\n",
    "    print(\"There are still NaNs\")\n",
    "else:\n",
    "    print(\"There are no more NaNs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine peaks/troughs data with track features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load position and radius features\n",
    "position_features_df = pd.read_csv(data_location + rf\"\\{date}_position_and_radius_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add position and radius features to all_peaks_and_troughs_df, on POSITION and TRACK_ID\n",
    "all_data = pd.merge(processed_peaks_and_troughs_df, position_features_df, on=[\"POSITION\", \"TRACK_ID\"])\n",
    "# Remove Unnamed columns\n",
    "all_data = all_data.loc[:, ~all_data.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all data\n",
    "all_data.to_csv(data_location + rf\"\\{date}_all_features_combined.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
