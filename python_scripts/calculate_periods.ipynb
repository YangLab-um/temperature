{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"09-20-24\"\n",
    "data_location = rf\"E:\\Project 6 - Temperature\\Experiments\\data_analysis\\{date}\"\n",
    "all_data = pd.read_csv(data_location + rf\"\\{date}_all_features_combined_renumbered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate full period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = pd.DataFrame()\n",
    "\n",
    "for position in all_data[\"POSITION\"].unique():\n",
    "    position_data = all_data[all_data[\"POSITION\"] == position]\n",
    "    for track_id in position_data[\"TRACK_ID\"].unique():\n",
    "        result_df = pd.DataFrame()\n",
    "        track_data = position_data[position_data[\"TRACK_ID\"] == track_id]\n",
    "        # Peak-to-peak time differences\n",
    "        peak_data = track_data[track_data[\"TYPE\"] == \"PEAK\"]\n",
    "        peak_data = peak_data.sort_values(\"TIME\")\n",
    "        peak_to_peak_period = peak_data[\"TIME\"].diff().dropna()\n",
    "        peak_to_peak_period = peak_to_peak_period.rename(\"PEAK_TO_PEAK_PERIOD\")\n",
    "        peak_cycle_numbers = peak_data[\"CYCLE\"].values[:-1]\n",
    "        # Trough-to-trough time differences\n",
    "        trough_data = track_data[track_data[\"TYPE\"] == \"TROUGH\"]\n",
    "        trough_data = trough_data.sort_values(\"TIME\")\n",
    "        trough_to_trough_period = trough_data[\"TIME\"].diff().dropna()\n",
    "        trough_to_trough_period = trough_to_trough_period.rename(\"TROUGH_TO_TROUGH_PERIOD\")\n",
    "        trough_cycle_numbers = trough_data[\"CYCLE\"].values[:-1]\n",
    "        # If there is a difference in series size, fill in the missing troughs with NaN\n",
    "        if len(peak_to_peak_period) > len(trough_to_trough_period):\n",
    "            for i in range(len(peak_to_peak_period) - len(trough_to_trough_period)):\n",
    "                trough_to_trough_period = pd.concat([trough_to_trough_period, pd.Series([np.nan])], ignore_index=True)\n",
    "                trough_cycle_numbers = np.append(trough_cycle_numbers, np.nan)\n",
    "        elif len(peak_to_peak_period) < len(trough_to_trough_period):\n",
    "            for i in range(len(trough_to_trough_period) - len(peak_to_peak_period)):\n",
    "                peak_to_peak_period = pd.concat([peak_to_peak_period, pd.Series([np.nan])], ignore_index=True)\n",
    "                peak_cycle_numbers = np.append(peak_cycle_numbers, np.nan)\n",
    "        # Add data to result dataframe\n",
    "        result_df[\"PEAK_TO_PEAK_PERIOD\"] = peak_to_peak_period.values\n",
    "        result_df[\"PEAK_TO_PEAK_CYCLE_NUMBER\"] = peak_cycle_numbers\n",
    "        result_df[\"TROUGH_TO_TROUGH_PERIOD\"] = trough_to_trough_period.values\n",
    "        result_df[\"TROUGH_TO_TROUGH_CYCLE_NUMBER\"] = trough_cycle_numbers\n",
    "        # Add track_id and position\n",
    "        result_df[\"TRACK_ID\"] = [track_id] * len(result_df)\n",
    "        result_df[\"POSITION\"] = [position] * len(result_df)\n",
    "        # Concatenate data\n",
    "        periods = pd.concat([periods, result_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add temperature information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = all_data.groupby([\"TRACK_ID\", \"POSITION\"])[\"MEAN_TEMPERATURE\"].mean().reset_index()\n",
    "periods = pd.merge(periods, temperature, on=[\"TRACK_ID\", \"POSITION\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods.to_csv(data_location + rf\"\\{date}_full_periods.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods[\"PEAK_TO_PEAK_CYCLE_NUMBER_STR\"] = periods[\"PEAK_TO_PEAK_CYCLE_NUMBER\"].astype(str)\n",
    "px.scatter(periods, x=\"MEAN_TEMPERATURE\", y=\"PEAK_TO_PEAK_PERIOD\", color=\"PEAK_TO_PEAK_CYCLE_NUMBER_STR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods[\"TROUGH_TO_TROUGH_CYCLE_NUMBER_STR\"] = periods[\"TROUGH_TO_TROUGH_CYCLE_NUMBER\"].astype(str)\n",
    "px.scatter(periods, x=\"MEAN_TEMPERATURE\", y=\"TROUGH_TO_TROUGH_PERIOD\", color=\"TROUGH_TO_TROUGH_CYCLE_NUMBER_STR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate peak-to-trough and trough-to-peak periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position to condition mapping\n",
    "def position_to_condition_mapping(position):\n",
    "    if position <= 13:\n",
    "        return 0\n",
    "    elif position >= 14 and position <= 27:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Condition to peak-trough order mapping\n",
    "condition_to_peak_trough_order_mapping = {\n",
    "    0: \"TROUGH-PEAK\",\n",
    "    1: \"PEAK-TROUGH\",\n",
    "    2: \"PEAK-TROUGH\"\n",
    "}\n",
    "\n",
    "partial_periods = pd.DataFrame()\n",
    "\n",
    "for position in all_data[\"POSITION\"].unique():\n",
    "    condition = position_to_condition_mapping(position)\n",
    "    peak_trough_order = condition_to_peak_trough_order_mapping[condition]\n",
    "    position_data = all_data[all_data[\"POSITION\"] == position]\n",
    "    for track_id in position_data[\"TRACK_ID\"].unique():\n",
    "        result_df = pd.DataFrame()\n",
    "        track_data = position_data[position_data[\"TRACK_ID\"] == track_id]\n",
    "        # Peak-to-trough time differences\n",
    "        peak_data = track_data[track_data[\"TYPE\"] == \"PEAK\"]\n",
    "        peak_data = peak_data.sort_values(\"TIME\")\n",
    "        trough_data = track_data[track_data[\"TYPE\"] == \"TROUGH\"]\n",
    "        trough_data = trough_data.sort_values(\"TIME\")\n",
    "        # See which cycles are present in both peak and trough data\n",
    "        peak_cycle_numbers = peak_data[\"CYCLE\"].values\n",
    "        trough_cycle_numbers = trough_data[\"CYCLE\"].values\n",
    "        common_cycles = np.intersect1d(peak_cycle_numbers, trough_cycle_numbers)\n",
    "        if len(common_cycles) == 0:\n",
    "            continue\n",
    "        # Find the time differences between the common cycles\n",
    "        peak_to_trough_period = []\n",
    "        for cycle in common_cycles:\n",
    "            try:\n",
    "                if peak_trough_order == \"PEAK-TROUGH\":\n",
    "                    peak_time = peak_data[peak_data[\"CYCLE\"] == cycle][\"TIME\"].values[0]\n",
    "                    trough_time = trough_data[trough_data[\"CYCLE\"] == cycle][\"TIME\"].values[0]\n",
    "                elif peak_trough_order == \"TROUGH-PEAK\":\n",
    "                    peak_time = peak_data[peak_data[\"CYCLE\"] == cycle][\"TIME\"].values[0]\n",
    "                    trough_time = trough_data[trough_data[\"CYCLE\"] == cycle + 1][\"TIME\"].values[0]\n",
    "            except IndexError:\n",
    "                continue\n",
    "            # peak_time = peak_data[peak_data[\"CYCLE\"] == cycle][\"TIME\"].values[0]\n",
    "            # trough_time = trough_data[trough_data[\"CYCLE\"] == cycle][\"TIME\"].values[0]\n",
    "            peak_to_trough_period.append(trough_time - peak_time)\n",
    "        if len(peak_to_trough_period) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            peak_to_trough_period = pd.Series(peak_to_trough_period)\n",
    "        # Trough-to-peak time differences\n",
    "        # We have to take the difference between the peak time of the next cycle and the trough time of the current cycle\n",
    "        trough_to_peak_period = []\n",
    "        trough_to_peak_cycle_numbers = []\n",
    "        for cycle in common_cycles:\n",
    "            try:\n",
    "                if peak_trough_order == \"PEAK-TROUGH\":\n",
    "                    peak_time = peak_data[peak_data[\"CYCLE\"] == cycle + 1][\"TIME\"].values[0]\n",
    "                    trough_time = trough_data[trough_data[\"CYCLE\"] == cycle][\"TIME\"].values[0]\n",
    "                elif peak_trough_order == \"TROUGH-PEAK\":\n",
    "                    peak_time = peak_data[peak_data[\"CYCLE\"] == cycle][\"TIME\"].values[0]\n",
    "                    trough_time = trough_data[trough_data[\"CYCLE\"] == cycle][\"TIME\"].values[0]\n",
    "                # peak_time = peak_data[peak_data[\"CYCLE\"] == cycle + 1][\"TIME\"].values[0]\n",
    "                # trough_time = trough_data[trough_data[\"CYCLE\"] == cycle][\"TIME\"].values[0]\n",
    "                trough_to_peak_period.append(peak_time - trough_time)\n",
    "                trough_to_peak_cycle_numbers.append(cycle)\n",
    "            except IndexError:\n",
    "                pass\n",
    "        if len(trough_to_peak_period) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            trough_to_peak_period = pd.Series(trough_to_peak_period)\n",
    "        # If there is a difference in series size, fill in the missing troughs with NaN\n",
    "        if len(peak_to_trough_period) > len(trough_to_peak_period):\n",
    "            for i in range(len(peak_to_trough_period) - len(trough_to_peak_period)):\n",
    "                trough_to_peak_period = pd.concat([trough_to_peak_period, pd.Series([np.nan])], ignore_index=True)\n",
    "                trough_to_peak_cycle_numbers = np.append(trough_to_peak_cycle_numbers, np.nan)\n",
    "        elif len(peak_to_trough_period) < len(trough_to_peak_period):\n",
    "            for i in range(len(trough_to_peak_period) - len(peak_to_trough_period)):\n",
    "                peak_to_trough_period = pd.concat([peak_to_trough_period, pd.Series([np.nan])], ignore_index=True)\n",
    "        # Add data to result dataframe\n",
    "        result_df[\"PEAK_TO_TROUGH_PERIOD\"] = peak_to_trough_period.values\n",
    "        result_df[\"TROUGH_TO_PEAK_PERIOD\"] = trough_to_peak_period.values\n",
    "        result_df[\"PEAK_TO_TROUGH_CYCLE_NUMBER\"] = common_cycles\n",
    "        result_df[\"TROUGH_TO_PEAK_CYCLE_NUMBER\"] = trough_to_peak_cycle_numbers\n",
    "        # Add track_id and position\n",
    "        result_df[\"TRACK_ID\"] = [track_id] * len(result_df)\n",
    "        result_df[\"POSITION\"] = [position] * len(result_df)\n",
    "        # Concatenate data\n",
    "        partial_periods = pd.concat([partial_periods, result_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add temperature information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = all_data.groupby([\"TRACK_ID\", \"POSITION\"])[\"MEAN_TEMPERATURE\"].mean().reset_index()\n",
    "partial_periods = pd.merge(partial_periods, temperature, on=[\"TRACK_ID\", \"POSITION\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any data points where the peak-to-trough or trough-to-peak period is negative\n",
    "# These correspond to mislabelled cycle numbers most likely\n",
    "negative_peak_to_trough = partial_periods[partial_periods[\"PEAK_TO_TROUGH_PERIOD\"] < 0]\n",
    "negative_trough_to_peak = partial_periods[partial_periods[\"TROUGH_TO_PEAK_PERIOD\"] < 0]\n",
    "partial_periods = partial_periods.drop(negative_peak_to_trough.index)\n",
    "partial_periods = partial_periods.drop(negative_trough_to_peak.index)\n",
    "partial_periods.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_periods.to_csv(data_location + rf\"\\{date}_partial_periods.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_periods[\"PEAK_TO_TROUGH_CYCLE_NUMBER_STR\"] = partial_periods[\"PEAK_TO_TROUGH_CYCLE_NUMBER\"].astype(str)\n",
    "px.scatter(partial_periods, x=\"MEAN_TEMPERATURE\", y=\"PEAK_TO_TROUGH_PERIOD\", color=\"PEAK_TO_TROUGH_CYCLE_NUMBER_STR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_periods[\"TROUGH_TO_PEAK_CYCLE_NUMBER_STR\"] = partial_periods[\"TROUGH_TO_PEAK_CYCLE_NUMBER\"].astype(str)\n",
    "px.scatter(partial_periods, x=\"MEAN_TEMPERATURE\", y=\"TROUGH_TO_PEAK_PERIOD\", color=\"TROUGH_TO_PEAK_CYCLE_NUMBER_STR\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
